{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "118535d1",
   "metadata": {},
   "source": [
    "# Primena neuronskih mreža"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0978b1e4",
   "metadata": {},
   "source": [
    "### Veštački Neuron i neuronska mreža\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "<img src=\"img/neuron.jpg\" style='width:90%; max-width:70rem'/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d272c6d4",
   "metadata": {},
   "source": [
    "## Aktivacione funkcije\n",
    "\n",
    "Uloga aktivacionih funkcija u neuronskoj mreži je da transformišu sumu ulaza neurona pomnoženih sa težinama u izlaznu vrednost koja se prosleđuje sledećem sloju.\n",
    "\n",
    "Korišćenje nelinearne aktivacione funkcije omogućava neuronskoj mreži da rešava nelinearne probleme.\n",
    "\n",
    "Ukoliko se koristi nelinearna aktivaciona funkcija u neuronima, dvoslojna neuronska mreža je u stanju da aproksimira bilo koju funkciju. Ovo je poznato kao **teorema o univerzalnoj aproksimaciji**.\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "<img src=\"img/sig.jpg\" style='width:90%; max-width:60rem'/>\n",
    "</div>\n",
    "\n",
    "Osobine\n",
    "* Izlaz su vrednosti u opsegu od 0 do 1\n",
    "* Često se koristi kada je potrebno predvideti verovatnoću\n",
    "\n",
    "Mane\n",
    "* Vanishing gradient problem\n",
    "  * Izvod funkcije je blizu nuli za vrednosti koje su mnogo udaljene od koordinatnong početka\n",
    "  * Mreža prestaje da uči kada se gradijenti približavaju nuli\n",
    "\n",
    "### Tanh (Hyperbolic Tangent)\n",
    "<div style=\"text-align:left\">\n",
    "<img src=\"img/tanh.jpg\" style='width:90%; max-width:60rem'/>\n",
    "</div>\n",
    "\n",
    "Osobine\n",
    "* Izlaz su vrednosti u opsegu od -1 do 1\n",
    "* Vrednosti su centrirane oko nule, pa je u stanju da mapira vrednosti kao jako negativne, neutralne, ili jako pozitivne.\n",
    "\n",
    "Mane\n",
    "* Vanishing gradient problem\n",
    "  * Izvod funkcije je blizu nuli za vrednosti koje su mnogo udaljene od koordinatnong početka\n",
    "  * Mreža prestaje da uči kada se gradijenti približavaju nuli\n",
    "\n",
    "### Relu (Rectified Linear Unit)\n",
    "<div style=\"text-align:left\">\n",
    "<img src=\"img/relu.jpg\" style='width:90%; max-width:60rem'/>\n",
    "</div>\n",
    "\n",
    "Osobine\n",
    "* Nelinearna funkcija u celini, ali linearna po delovima\n",
    "* Neuron je deaktiviran samo ako je rezultat linearne transformacije u neuronu manji od 0\n",
    "* Laka za izračuvananje\n",
    "* Najčešće se koristi u skrivenim slojevima jer izbegava vanishing gradient problem\n",
    "\n",
    "Mane\n",
    "* Dying ReLU\n",
    "  * Leva strana funkcije ima izvod 0, i zbog toga se težine nekih neurona ne ažuriraju u backpropagation procesu\n",
    "  * Ovo može da stvori mrtve neurone koji se nikada ne aktiviraju\n",
    "\n",
    "### Linear\n",
    "<div style=\"text-align:left\">\n",
    "<img src=\"img/linear.jpg\" style='width:90%; max-width:50rem'/>\n",
    "</div>\n",
    "\n",
    "Osobine\n",
    "* Funkcija Identiteta\n",
    "* Samo vraća vrednost koja joj je prosleđena\n",
    "* Čest izbor za aktivacionu funkciju poslednjeg izlaznog sloja kod problema regresije\n",
    "\n",
    "Mane\n",
    "* Ne treba da se koristi u skrivenim slojevima\n",
    "  * Mreža koja koristi samo linearnu aktivacionu funkciju neće biti u stanju da nauči nelinearne funkcije\n",
    "\n",
    "### Softmax\n",
    "<div style=\"text-align:left\">\n",
    "<img src=\"img/softmax.jpg\" style='width:90%; max-width:60rem'/>\n",
    "</div>\n",
    "\n",
    "Osobine\n",
    "* Može se posmatrati kao kombinacija više sigmoidnih funkcija\n",
    "* Računa relativne verovatnoće klasa, i vraća verovatnoću za svaku klasu\n",
    "* Najčešće se koristi kod multi-class klasifikacije u poslednjem sloju\n",
    "\n",
    "\n",
    "## Vanishing i Exploding Gradients problemi\n",
    "\n",
    "Ukoliko su ulazne vrednosti za aktivacionu funkciju previše udaljene od 0, tada neke aktivacione funkcije kao što su sigmoidna i tanh mogu postati saturirane, i njihov gradijent je tada blizu 0. Ukoliko je mreža duboka, ovako male vrednosti gradijenata mogu usporiti brzinu učenja, i to se naziva **Vanishing gradient** problem.\n",
    "\n",
    "**Exploding gradient** problem nastaje kada vrednosti koje neuroni izbacuju postanu previše velike zbog eksponencijalnog rasta parametara modela. Ovo može dovesti do toga da parametri u modelu dobiju NaN vrednosti zbog toga što se može desiti overflow.\n",
    "\n",
    "Neke od tehnika za rešavanje ovih problema:\n",
    "* Inicijalizacija težina\n",
    "  * Najčešće se koristi [Xavier](https://paperswithcode.com/method/xavier-initialization) inicijalizacija\n",
    "* Korišćenje aktivacionih funkcija koje nemaju problem sa saturacijom\n",
    "  * Najšečće se koristi ReLU\n",
    "* Batch Normalization\n",
    "  * Normalizacija težina u slojevima neuronske mreže\n",
    "* Gradient Clipping\n",
    "  * Gradijenti iznad nekog praga se postavljaju na vrednost praga\n",
    "\n",
    "## Odabir adekvatne aktivacione funkcije\n",
    "\n",
    "Za skrivene slojeve, dobra polazna aktivaciona funkcija je **ReLU**.\n",
    "\n",
    "U zavisnosti od vrste problema, bira se aktivaciona funkcija u izlaznom sloju.\n",
    "* Regresija - Linearna aktivaciona funkcija\n",
    "* Binarna klasifikacija - Sigmoidna aktivaciona funkcija\n",
    "* Multiclass klasifikacija - Softmax\n",
    "  * Podatak može pripadati samo jednoj od ponuđenih klasa\n",
    "* Multilabel klasifikacija - Sigmoidna aktivaciona funkcija\n",
    "  * Podatak može imati više labela\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "<img src=\"img/biranje-class-reg.jpg\" style='width:90%; max-width:60rem'/>\n",
    "</div>\n",
    "\n",
    "## Podela skupa podataka\n",
    "\n",
    "Skup podataka na kojem se primenjuju neuronske mreže se najčešće ne koristi ceo za treniranje. U praksi se skup podataka deli na **trening, validacioni i test** skup (validacioni skup je koristan kada treba podešavati hiperparametre). Za potrebe ovog predmeta, dovoljno će biti da se u zadacima izdvoje **trening i test** skup.\n",
    "\n",
    "Svrha testnog skupa je da nakon treniranja neuronske mreže na trening skupu, možemo da vidimo kakve bi performanse model mogao da ima na podacima koje možda nije video prilikom treniranja. Ovo nam može dati uvid u to kako će se model ponašati kada se primeni u praksi. Idealno se neuronska mreža evaluira samo jednom na testnom skupu, da bi se dobila što realnija procena performansi.\n",
    "\n",
    "Ne postoji tačno pravilo koje kaže koliko podataka treba da bude u trening i test skupu.\n",
    "Ali u praksi se često prate sledeće smernice:\n",
    "* Kada je broj podataka manji od 10000, često se koristi **70/30** podela, gde 70% podataka pripada trening skupu, a ostatak test skupu.\n",
    "* Za velike skupove podataka se ovaj odnos menja, i često se koristi **95/5** podela.\n",
    "\n",
    "## Bias i varijansa\n",
    "\n",
    "Na primeru klasifikacije gde treba prepoznati da li se na slici nalazi mačka, čovek može postići savršen rezultat sa greškom od 0%.\n",
    "Za korišćenje modela mašinskog učenja, možemo podeliti ovaj skup podataka na trening i test skup.\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "<img src=\"img/high-bias-and-variance.jpg\" style='width:90%; max-width:60rem'/>\n",
    "</div>\n",
    "\n",
    "* Bias je razlika između greške modela na trening skupu i najmanje moguće greške\n",
    "* Varijansa je razlika između grešaka na testnom skupu i trening skupu\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "<img src=\"img/high-variance.jpg\" style='width:90%; max-width:60rem'/>\n",
    "</div>\n",
    "\n",
    "Ukoliko naš model ima sledeći odnos grešaka:\n",
    "* Trening greška = 1%\n",
    "* Test greška = 12%\n",
    "\n",
    "Iz grešaka na trening skupu se vidi da se model dobro prilagodio trening skupu podataka.\n",
    "\n",
    "Razlika u odnosu na ljudske performanse je 1% = (1% - 0%). Zbog toga model ima **mali bias**.\n",
    "\n",
    "Ali razlika između greške na testnom i trening skupu je 11%=(12%-1%). Zbog toga model ima **veliku varijansu**.\n",
    "\n",
    "Mali bias i velika varijansa ukazuju na to da model nije naučio dobro da generalizuje, i kaže se da se desio **overfitting**.\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "<img src=\"img/high-bias.jpg\" style='width:90%; max-width:60rem'/>\n",
    "</div>\n",
    "\n",
    "Ukoliko naš model ima sledeći odnos grešaka:\n",
    "* Trening greška = 14%\n",
    "* Test greška = 15%\n",
    "\n",
    "U ovom primeru je **bias velik** i iznosi 14%=(14% - 0%).\n",
    "\n",
    "**Varijansa je ovde mala** i iznosi 1%=(15% - 14%).\n",
    "\n",
    "To nam govori da je ostalo još dosta prostora da se performanse modela unaprede, tako što će se bolje prilagoditi na trening skup.\n",
    "\n",
    "U ovom slučaju kažemo da se desio **underfitting**.\n",
    "\n",
    "### Tehnike za smanjenje biasa i varijanse\n",
    "\n",
    "Idealno bi želeli da model ima mali bias i malu varijansu.\n",
    "\n",
    "Za **smanjenje varijanse** i da bi se **umanjio overfitting**:\n",
    "* Dodati više podataka u trening skup\n",
    "* Smanjiti fleksibilnost modela\n",
    "  * Smanjiti broj parametara modela smanjenjem broja slojeva ili broja neurona u slojevima\n",
    "* Uvesti regularizaciju\n",
    "  * U neuronskim mrežama se često koristi **dropout regularizacija** gde se nasumično prilikom treniranja deaktivira zadati procenat neurona\n",
    "\n",
    "Za **smanjenje biasa** i da bi se **umanjio underfitting**:\n",
    "* Povećati fleksibilnost modela\n",
    "  * Povećati broj parametara modela uvećanjem broja slojeva ili broja neurona u slojevima\n",
    "* Smanjiti regularizaciju\n",
    "\n",
    "\n",
    "## Normalizacija i Standardizacija\n",
    "\n",
    "Pre korišćenja neuronske mreže na podacima, potrebno je podatke standardizovati ili normalizovati.\n",
    "Ovo se radi da bi se izbegao **exploding gradients** problem kojem mogu doprineti velike vrednosti na ulazu u mrežu.\n",
    "Takođe, zbog velike razlike u skali ulaznih vrednosti koje nisu normalizovane ili standardizovane, može se desiti da se mreža sporo trenira, jer algoritmu gradijentnog spusta treba puno vremena da konvergira.\n",
    "\n",
    "\n",
    "### Normalizacija\n",
    "\n",
    "Normalizacija predstavlja maprianje svih vrednosti na opseg [0, 1].\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "<img src=\"img/norm.jpg\" style='width:90%; max-width:25rem'/>\n",
    "</div>\n",
    "\n",
    "### Standardizacija (Z-Normalizacija)\n",
    "\n",
    "Standardizacija je transformacija podataka u kojoj se od svake vrednosti oduzima srednja vrednost i deli se sa standardnom devijacijom.\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "<img src=\"img/std.jpg\" style='width:90%; max-width:25rem'/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fe3dce6",
   "metadata": {},
   "source": [
    "## Petlja za treniranje\n",
    "\n",
    "Ovo je primer pojednostavljene petlje za treniranje.\n",
    "\n",
    "Prilikom treniranja, potrebno je prvo napraviti model koji će se trenirati.\n",
    "\n",
    "Zatim se definišu hiperparametri kao što je broj epoha i learning rate.\n",
    "\n",
    "Epohe predstavljaju broj epizoda u kojima će se model trenirati.\n",
    "\n",
    "U svakoj epohi se podaci propuštaju kroz model, i model vraća predikcije. Zatim se računa greška koju model pravi na podacima. Ta greška se koristi da bi se izračunali gradijenti i da bi se na kraju ažurirale težine u samom modelu.\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "<img src=\"img/training.jpg\" style='width:90%; max-width:35rem'/>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd0f3756",
   "metadata": {},
   "source": [
    "## Zadatak 1\n",
    "\n",
    "U zadatku se koristi life-expectancy.csv skup podataka. Potrebno je predvitedi očekivanu dužinu života na osnovu ostalih obeležja u skupu podataka.\n",
    "\n",
    "Zadatak raditi u zadatak1.py fajlu.\n",
    "\n",
    "* TODO preprocesirati podatke\n",
    "* TODO podeliti podatke na train i test skup\n",
    "* TODO razdvojiti x i y\n",
    "* TODO primeniti standardizaciju ili normalizaciju\n",
    "* TODO implementirati model neuronske mreze\n",
    "* TODO trenirati mrezu\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6380de4d",
   "metadata": {},
   "source": [
    "## Zadatak 2\n",
    "\n",
    "U zadatku se koristi diabetes.csv skup podataka. Potrebno je predvitedi da li osoba ima dijabetes na osnovu ostalih obeležja u skupu podataka.\n",
    "\n",
    "Zadatak raditi u zadatak2.py fajlu\n",
    "\n",
    "* TODO preprocesirati podatke\n",
    "* TODO podeliti podatke na train i test skup\n",
    "* TODO razdvojiti x i y\n",
    "* TODO primeniti standardizaciju ili normalizaciju\n",
    "* TODO implementirati model neuronske mreze\n",
    "* TODO trenirati mrezu - koristiti sklearn.neural_network.MLPClassifier "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e1fa606",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
